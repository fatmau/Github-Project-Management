{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zenhub & Github Issue Import and Formatting\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#functions\">Functions</a></li>\n",
    "<li><a href=\"#exploration\">Data Exploration</a></li>\n",
    "<li><a href=\"#csv\">Writing to CSV</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "Before process starts, first import the necessary libraries and initialise global parameters.\n",
    "\n",
    "* Project parameters could be find under project settings.\n",
    "* For Github token, go to `Settings \\ Developer settings \\ Personal access tokens` to generate a new token.\n",
    "* For Zenhub token, go to https://app.zenhub.com/dashboard/tokens and generate a new token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "########## SET BELOW PARAMETERS ##########\n",
    "# Project Parameters\n",
    "OWNER = \"\"\n",
    "REPOSITORY_ID = \"\"\n",
    "REPOSITORY_NAME = \"\"\n",
    "\n",
    "# API TOKENS\n",
    "ZENHUB_API_TOKEN = \"\"\n",
    "GITHUB_API_TOKEN = \"\"\n",
    "\n",
    "# URLS\n",
    "ZENHUB_REPO_URL = \"https://.../p1/repositories/\" # url ends with \"/repositories/\"\n",
    "GITHUB_REPO_URL = \"https://.../api/v3/repos/\" # url ends with \"/repos/\"\n",
    "########## SET ABOVE PARAMETERS ##########\n",
    "\n",
    "\n",
    "# Zenhub URL Parameters\n",
    "ZENHUB_URL_EPICS = (ZENHUB_REPO_URL + REPOSITORY_ID + \"/epics\")\n",
    "ZENHUB_URL_ISSUES = (ZENHUB_REPO_URL + REPOSITORY_ID + \"/issues/\")\n",
    "ZENHUB_HEADERS = {\"X-Authentication-Token\": ZENHUB_API_TOKEN}\n",
    "\n",
    "# Github URL Parameters\n",
    "GITHUB_ISSUES_URL = (GITHUB_REPO_URL + OWNER + \"/\" + REPOSITORY_NAME + \"/issues\")\n",
    "GITHUB_HEADERS = {\"Authorization\": (\"Bearer \" + GITHUB_API_TOKEN)}\n",
    "\n",
    "# APIs Rate Limit waiting time\n",
    "SLEEP_TIME = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='functions'></a>\n",
    "## Functions\n",
    "\n",
    "In this section, functions will be defined\n",
    "\n",
    "* get_epic_issues_json\n",
    "* get_issue_events_json\n",
    "* get_issue_labels_and_type\n",
    "* get_all_issues\n",
    "* get_event_df\n",
    "* calculate_time_spent_per_column\n",
    "* insert_first_date_per_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function returning all issues under an epic in json format\n",
    "def get_epic_issues_json(epic):\n",
    "\n",
    "    # edit URL to get a single epic\n",
    "    temp_issues_url = ZENHUB_URL_EPICS + \"/\" + epic\n",
    "\n",
    "    # call the API endpoint and sleep 60sec if call returns API Limit exception\n",
    "    issues_response = requests.get(temp_issues_url, headers=ZENHUB_HEADERS)\n",
    "    if issues_response.status_code == 403:\n",
    "        time.sleep(SLEEP_TIME)\n",
    "        issues_response = requests.get(temp_issues_url, headers=ZENHUB_HEADERS)\n",
    "\n",
    "    return issues_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function returning all events happened in the lifetime of an issue in json format\n",
    "def get_issue_events_json(issue_id):\n",
    "\n",
    "    # edit URL to get issues events\n",
    "    temp_events_url = ZENHUB_URL_ISSUES + issue_id + \"/events\"\n",
    "\n",
    "    # call the API endpoint and sleep 60sec if call returns API Limit exception\n",
    "    events_response = requests.get(temp_events_url, headers=ZENHUB_HEADERS)\n",
    "    if events_response.status_code == 403:\n",
    "        time.sleep(SLEEP_TIME)\n",
    "        events_response = requests.get(temp_events_url, headers=ZENHUB_HEADERS)\n",
    "\n",
    "    return events_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function returning label set and type by parsing a json input\n",
    "def get_issue_labels_and_type(labels_json):\n",
    "\n",
    "    # parse the json and get the labels list\n",
    "    labels = \"\"\n",
    "    for label in labels_json:\n",
    "        if labels == \"\":\n",
    "            labels = label[\"name\"]\n",
    "        else:\n",
    "            labels = labels + \",\" + label[\"name\"]\n",
    "\n",
    "    # find the type of the story by looking at the label\n",
    "    issue_type = \"\"\n",
    "    if labels.find(\"story\") != -1:\n",
    "        issue_type = \"story\"\n",
    "    elif labels.find(\"bug\") != -1:\n",
    "        issue_type = \"bug\"\n",
    "    elif labels.find(\"discovery\") != -1:\n",
    "        issue_type = \"discovery\"\n",
    "    elif labels.find(\"spike\") != -1:\n",
    "        issue_type = \"spike\"\n",
    "    elif labels.find(\"setup\") != -1:\n",
    "        issue_type = \"setup\"\n",
    "    elif labels.find(\"security\") != -1:\n",
    "        issue_type = \"security\"\n",
    "    elif labels.find(\"tech-debt\") != -1:\n",
    "        issue_type = \"techdebt\"\n",
    "    elif labels.find(\"documentation\") != -1:\n",
    "        issue_type = \"documentation\"\n",
    "\n",
    "    return labels, issue_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function returning all issues under your Github repository in json format\n",
    "def get_all_issues(page_number):\n",
    "\n",
    "    # call the API endpoint and sleep 60sec if call returns API Limit exception\n",
    "    issues_response = requests.get(\n",
    "        GITHUB_ISSUES_URL, headers=GITHUB_HEADERS, params={\"page\": str(page_number)}\n",
    "    )\n",
    "    if issues_response.status_code == 403:\n",
    "        time.sleep(SLEEP_TIME)\n",
    "        issues_response = requests.get(\n",
    "            GITHUB_ISSUES_URL, headers=GITHUB_HEADERS, params={\"page\": str(page_number)}\n",
    "        )\n",
    "\n",
    "    return issues_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function parsing json and returning issue events in dataframe format\n",
    "def get_event_df(events_json):\n",
    "\n",
    "    # initialise the dataframe first\n",
    "    df_events_col_names = [\"from_pipeline\", \"to_pipeline\", \"created_at\"]\n",
    "    df_events = pd.DataFrame(columns=df_events_col_names)\n",
    "\n",
    "    # parse json and append data to dataframe\n",
    "    for event in events_json:\n",
    "\n",
    "        # for kanban board issue transfer events, we will use the events with \"transferIssue\" type\n",
    "        if event[\"type\"] == \"transferIssue\":\n",
    "            from_pipeline = event[\"from_pipeline\"][\"name\"]\n",
    "            to_pipeline = event[\"to_pipeline\"][\"name\"]\n",
    "            created_at = datetime.datetime.strptime(\n",
    "                event[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
    "            ).date()\n",
    "            df_events = df_events.append(\n",
    "                {\n",
    "                    \"from_pipeline\": from_pipeline,\n",
    "                    \"to_pipeline\": to_pipeline,\n",
    "                    \"created_at\": created_at,\n",
    "                },\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return df_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function calculating times spent on each column of your kanban board (except 'Done' column)\n",
    "def calculate_time_spent_per_column(df_events, created_at):\n",
    "    \n",
    "    # initialise parameters\n",
    "    new_issues = tech_debt = spike = in_analysis = ready_to_pick = in_progress = sign_off = done = 0\n",
    "    prev_date = created_at\n",
    "    \n",
    "    # iterate on events dataframe\n",
    "    df_events = df_events.sort_values(by=['created_at'])\n",
    "    for row in df_events.iterrows():\n",
    "        \n",
    "        from_pipeline = row[1]['from_pipeline']\n",
    "        action_date = row[1]['created_at']\n",
    "        delta = (action_date - prev_date).days # 2 moves happened at the same day, delta is 0!!!\n",
    "        prev_date = row[1]['created_at']\n",
    "    \n",
    "        # calculate \"how many days a ticket stand in each column\"\n",
    "        if (from_pipeline == \"New Issues\"):\n",
    "            new_issues = new_issues + delta\n",
    "        elif (from_pipeline == \"Tech Debt\"):\n",
    "            tech_debt = tech_debt + delta\n",
    "        elif (from_pipeline.startswith(\"Spike\")):\n",
    "            spike = spike + delta\n",
    "        elif (from_pipeline == \"In analysis\"):\n",
    "            in_analysis = in_analysis + delta\n",
    "        elif (from_pipeline == \"Ready to pick\"):\n",
    "            ready_to_pick = ready_to_pick + delta\n",
    "        elif (from_pipeline == \"In Progress\"):\n",
    "            in_progress = in_progress + delta\n",
    "        elif (from_pipeline == \"Sign off\"):\n",
    "            sign_off = sign_off + delta\n",
    "    \n",
    "    return new_issues, tech_debt, spike, in_analysis, ready_to_pick, in_progress, sign_off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function finding first date when an issue moved to a column on your kanban board (except 'New Issues' column)\n",
    "# NOTE: an issue can be moved to a column multiple times, this function will find the date for only the first times\n",
    "def insert_first_date_per_column(df_events, df_issues):\n",
    "\n",
    "    # iterate on events dataframe\n",
    "    df_events = df_events.sort_values(by=[\"created_at\"], ascending=False)\n",
    "    for row in df_events.iterrows():\n",
    "        to_pipeline = row[1][\"to_pipeline\"]\n",
    "        action_date = row[1][\"created_at\"]\n",
    "\n",
    "        # find \"ticket's first date in each column\"\n",
    "        if to_pipeline == \"Tech Debt\":\n",
    "            df_issues.loc[df_issues.issue_id == issue_id, \"tech_debt_fd\"] = action_date\n",
    "        elif to_pipeline.startswith(\"Spike\"):\n",
    "            df_issues.loc[df_issues.issue_id == issue_id, \"spike_fd\"] = action_date\n",
    "        elif to_pipeline == \"In analysis\":\n",
    "            df_issues.loc[\n",
    "                df_issues.issue_id == issue_id, \"in_analysis_fd\"\n",
    "            ] = action_date\n",
    "        elif to_pipeline == \"Ready to pick\":\n",
    "            df_issues.loc[\n",
    "                df_issues.issue_id == issue_id, \"ready_to_pick_fd\"\n",
    "            ] = action_date\n",
    "        elif to_pipeline == \"In Progress\":\n",
    "            df_issues.loc[\n",
    "                df_issues.issue_id == issue_id, \"in_progress_fd\"\n",
    "            ] = action_date\n",
    "        elif to_pipeline == \"Sign off\":\n",
    "            df_issues.loc[df_issues.issue_id == issue_id, \"sign_off_fd\"] = action_date\n",
    "        elif to_pipeline == \"Done\":\n",
    "            df_issues.loc[df_issues.issue_id == issue_id, \"done_fd\"] = action_date\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## Data Exploration\n",
    "\n",
    "Time for the real job: issues dataframe initialisation, and filling with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the issues dataframe\n",
    "col_names = [\n",
    "    \"epic_id\",\n",
    "    \"issue_id\",\n",
    "    \"title\",\n",
    "    \"labels\",\n",
    "    \"issue_type\",\n",
    "    \"state\",\n",
    "    \"created_at\",\n",
    "    \"events\",\n",
    "    \"new_issues\",\n",
    "    \"tech_debt\",\n",
    "    \"spike\",\n",
    "    \"in_analysis\",\n",
    "    \"ready_to_pick\",\n",
    "    \"in_progress\",\n",
    "    \"sign_off\",\n",
    "    \"tech_debt_fd\",\n",
    "    \"spike_fd\",\n",
    "    \"in_analysis_fd\",\n",
    "    \"ready_to_pick_fd\",\n",
    "    \"in_progress_fd\",\n",
    "    \"sign_off_fd\",\n",
    "    \"done_fd\",\n",
    "]\n",
    "df_issues = pd.DataFrame([], columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over!\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THIS CELL'S EXECUTION CAN TAKE A WHILE (minutes) DUE TO API LIMIT SLEEPING!!!!\n",
    "\n",
    "# all functions are defined and initialisations made\n",
    "# now we will get all issues on our github repo by pagination\n",
    "page_number = 1\n",
    "issues_json = get_all_issues(page_number)\n",
    "\n",
    "# continue since next page of API response is empty!\n",
    "while len(issues_json) > 0:\n",
    "\n",
    "    ## loop on issues at a response\n",
    "    for issue in issues_json:\n",
    "\n",
    "        # get issue parameters\n",
    "        issue_id = str(issue[\"number\"])\n",
    "        created_at = datetime.datetime.strptime(\n",
    "            issue[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "        ).date()\n",
    "        state = issue[\"state\"]\n",
    "        title = issue[\"title\"]\n",
    "        labels, issue_type = get_issue_labels_and_type(issue[\"labels\"])\n",
    "\n",
    "        # get events json and events data frame of issue\n",
    "        events_json = get_issue_events_json(issue_id)\n",
    "        df_events = get_event_df(events_json)\n",
    "\n",
    "        # get delta calculations\n",
    "        (\n",
    "            new_issues,\n",
    "            tech_debt,\n",
    "            spike,\n",
    "            in_analysis,\n",
    "            ready_to_pick,\n",
    "            in_progress,\n",
    "            sign_off,\n",
    "        ) = calculate_time_spent_per_column(df_events, created_at)\n",
    "\n",
    "        # insert issue into issues data frame\n",
    "        df_issues = df_issues.append(\n",
    "            {\n",
    "                \"epic_id\": \"\",\n",
    "                \"issue_id\": issue_id,\n",
    "                \"title\": title,\n",
    "                \"labels\": labels,\n",
    "                \"issue_type\": issue_type,\n",
    "                \"state\": state,\n",
    "                \"created_at\": created_at,\n",
    "                \"events\": df_events.to_json(date_format=\"iso\"),\n",
    "                \"new_issues\": new_issues,\n",
    "                \"tech_debt\": tech_debt,\n",
    "                \"spike\": spike,\n",
    "                \"in_analysis\": in_analysis,\n",
    "                \"ready_to_pick\": ready_to_pick,\n",
    "                \"in_progress\": in_progress,\n",
    "                \"sign_off\": sign_off,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        # add \"first date\" when a ticket moved to a column\n",
    "        insert_first_date_per_column(df_events, df_issues)\n",
    "\n",
    "    # get next page's json file\n",
    "    page_number = page_number + 1\n",
    "    issues_json = get_all_issues(page_number)\n",
    "\n",
    "print(\"over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over!\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THIS CELL'S EXECUTION CAN TAKE A WHILE (minutes) DUE TO API LIMIT SLEEPING!!!!\n",
    "\n",
    "# now we will get all epics on our github repo\n",
    "epic_list = []\n",
    "json_content = requests.get(ZENHUB_URL_EPICS, headers=ZENHUB_HEADERS).json()\n",
    "for epic in json_content[\"epic_issues\"]:\n",
    "    epic_id = str(epic[\"issue_number\"])\n",
    "\n",
    "    # get all issues assigned to that epic, and update epic id of issue at data frame\n",
    "    issues_json = get_epic_issues_json(epic_id)\n",
    "    for issue in issues_json[\"issues\"]:\n",
    "        issue_id = str(issue[\"issue_number\"])\n",
    "        df_issues.loc[df_issues.issue_id == issue_id, \"epic_id\"] = epic_id\n",
    "\n",
    "print(\"over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epic_id</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>title</th>\n",
       "      <th>labels</th>\n",
       "      <th>issue_type</th>\n",
       "      <th>state</th>\n",
       "      <th>created_at</th>\n",
       "      <th>events</th>\n",
       "      <th>new_issues</th>\n",
       "      <th>tech_debt</th>\n",
       "      <th>...</th>\n",
       "      <th>ready_to_pick</th>\n",
       "      <th>in_progress</th>\n",
       "      <th>sign_off</th>\n",
       "      <th>tech_debt_fd</th>\n",
       "      <th>spike_fd</th>\n",
       "      <th>in_analysis_fd</th>\n",
       "      <th>ready_to_pick_fd</th>\n",
       "      <th>in_progress_fd</th>\n",
       "      <th>sign_off_fd</th>\n",
       "      <th>done_fd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>439</td>\n",
       "      <td>Retire /filter-auto-populate-values</td>\n",
       "      <td>story 🍰</td>\n",
       "      <td>story</td>\n",
       "      <td>open</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>{\"from_pipeline\":{\"0\":\"New Issues\"},\"to_pipeli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>406</td>\n",
       "      <td>438</td>\n",
       "      <td>Reduce nodes to 2 for Redshift TEST cluster</td>\n",
       "      <td>setup ⚙️,tech-debt 💸</td>\n",
       "      <td>setup</td>\n",
       "      <td>open</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>{\"from_pipeline\":{\"0\":\"Sign off\",\"1\":\"Ready to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>2020-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>435</td>\n",
       "      <td>Update d_merchant_extended ingestion date to y...</td>\n",
       "      <td>story 🍰,tech-debt 💸</td>\n",
       "      <td>story</td>\n",
       "      <td>open</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>{\"from_pipeline\":{\"0\":\"In analysis\",\"1\":\"New I...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  epic_id issue_id                                              title  \\\n",
       "0     225      439                Retire /filter-auto-populate-values   \n",
       "1     406      438        Reduce nodes to 2 for Redshift TEST cluster   \n",
       "2      71      435  Update d_merchant_extended ingestion date to y...   \n",
       "\n",
       "                 labels issue_type state  created_at  \\\n",
       "0               story 🍰      story  open  2020-03-26   \n",
       "1  setup ⚙️,tech-debt 💸      setup  open  2020-03-26   \n",
       "2   story 🍰,tech-debt 💸      story  open  2020-03-24   \n",
       "\n",
       "                                              events new_issues tech_debt  \\\n",
       "0  {\"from_pipeline\":{\"0\":\"New Issues\"},\"to_pipeli...          0         0   \n",
       "1  {\"from_pipeline\":{\"0\":\"Sign off\",\"1\":\"Ready to...          0         0   \n",
       "2  {\"from_pipeline\":{\"0\":\"In analysis\",\"1\":\"New I...          0         0   \n",
       "\n",
       "   ... ready_to_pick in_progress sign_off tech_debt_fd spike_fd  \\\n",
       "0  ...             0           0        0          NaN      NaN   \n",
       "1  ...             0           0        0          NaN      NaN   \n",
       "2  ...             0           0        0          NaN      NaN   \n",
       "\n",
       "  in_analysis_fd ready_to_pick_fd in_progress_fd sign_off_fd     done_fd  \n",
       "0     2020-03-26              NaN            NaN         NaN         NaN  \n",
       "1            NaN       2020-03-26            NaN  2020-03-26  2020-03-26  \n",
       "2     2020-03-24       2020-03-26            NaN         NaN         NaN  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_issues.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epic_id</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>title</th>\n",
       "      <th>labels</th>\n",
       "      <th>issue_type</th>\n",
       "      <th>state</th>\n",
       "      <th>created_at</th>\n",
       "      <th>events</th>\n",
       "      <th>new_issues</th>\n",
       "      <th>tech_debt</th>\n",
       "      <th>...</th>\n",
       "      <th>ready_to_pick</th>\n",
       "      <th>in_progress</th>\n",
       "      <th>sign_off</th>\n",
       "      <th>tech_debt_fd</th>\n",
       "      <th>spike_fd</th>\n",
       "      <th>in_analysis_fd</th>\n",
       "      <th>ready_to_pick_fd</th>\n",
       "      <th>in_progress_fd</th>\n",
       "      <th>sign_off_fd</th>\n",
       "      <th>done_fd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>...</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>176</td>\n",
       "      <td>198</td>\n",
       "      <td>247</td>\n",
       "      <td>179</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>23</td>\n",
       "      <td>352</td>\n",
       "      <td>352</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>320</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>51</td>\n",
       "      <td>66</td>\n",
       "      <td>105</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td>92</td>\n",
       "      <td>SKU Insights API - data materialization &amp; inge...</td>\n",
       "      <td>story 🍰</td>\n",
       "      <td>story</td>\n",
       "      <td>open</td>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>{\"from_pipeline\":{\"0\":\"New Issues\"},\"to_pipeli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>2019-10-18</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2020-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>127</td>\n",
       "      <td>184</td>\n",
       "      <td>354</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>284</td>\n",
       "      <td>340</td>\n",
       "      <td>...</td>\n",
       "      <td>206</td>\n",
       "      <td>164</td>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       epic_id issue_id                                              title  \\\n",
       "count      354      354                                                354   \n",
       "unique      23      352                                                352   \n",
       "top                  92  SKU Insights API - data materialization & inge...   \n",
       "freq        99        2                                                  2   \n",
       "\n",
       "         labels issue_type state  created_at  \\\n",
       "count       354        354   354         354   \n",
       "unique       33          9     1          86   \n",
       "top     story 🍰      story  open  2019-09-18   \n",
       "freq        127        184   354          16   \n",
       "\n",
       "                                                   events  new_issues  \\\n",
       "count                                                 354         354   \n",
       "unique                                                320          28   \n",
       "top     {\"from_pipeline\":{\"0\":\"New Issues\"},\"to_pipeli...           0   \n",
       "freq                                                    5         284   \n",
       "\n",
       "        tech_debt  ...  ready_to_pick  in_progress  sign_off  tech_debt_fd  \\\n",
       "count         354  ...            354          354       354            30   \n",
       "unique         13  ...             31           28        19            21   \n",
       "top             0  ...              0            0         0    2020-03-10   \n",
       "freq          340  ...            206          164       187             3   \n",
       "\n",
       "          spike_fd in_analysis_fd ready_to_pick_fd in_progress_fd sign_off_fd  \\\n",
       "count           22            176              198            247         179   \n",
       "unique          14             51               66            105          90   \n",
       "top     2019-10-18     2020-02-26       2020-02-03     2019-09-18  2020-03-25   \n",
       "freq             6             17               12              9           6   \n",
       "\n",
       "           done_fd  \n",
       "count          268  \n",
       "unique          90  \n",
       "top     2020-02-17  \n",
       "freq            18  \n",
       "\n",
       "[4 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_issues.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='csv'></a>\n",
    "## Writing to CSV\n",
    "\n",
    "It is time to write issues data into a CSV for external uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues.to_csv(\"github_issues.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "zenhub_lead_times",
  "notebookId": 2536496
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
